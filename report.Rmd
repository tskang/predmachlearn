---
title: "Practical Machine Learning Project"
author: Tae Seung Kang
date: "October 25, 2015"
output:
  html_document:
    fig_height: 9
    fig_width: 9
  pdf_document:
    fig_height: 9
    fig_width: 9
---

## Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. In this report, we describe how we built our model, how we used cross validation, what the expected out of sample error is, and why we made the choices we did. 


## Data Preparation

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

```{r, cache = T, echo=F}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)

os <- .Platform$OS.type
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" # for linux
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" # for linux
if(os == "windows") {
  trainUrl <-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" # for windows
  testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" # for windows
} 
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  if(os == "windows") 
    download.file(trainUrl, destfile=trainFile, method = "internal") # for windows
  else
    download.file(trainUrl, destfile=trainFile, method="curl") # for linux
}
if (!file.exists(testFile)) {
  if(os == "windows") 
    download.file(testUrl, destfile=testFile, method = "internal") # for windows
  else
    download.file(testUrl, destfile=testFile, method="curl") # for linux
}
```  

After downloading, we load the training and testing data sets assuming they are in directory "./data".
```{r, cache = T}
trainset <- read.csv(trainFile)
testset <- read.csv(testFile)
dim(trainset)
dim(testset)
```
The training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables. The "classe" variable in the testing set is the outcome to predict.

### Data Cleaning
In this step, we clean the data and get rid of missing values as well as some meaningless variables.
```{r, cache = T, echo=F, results="hide"}
sum(complete.cases(trainset))
```
First, we remove the columns that contain missing values NA.
```{r, cache = T}
trainset <- trainset[, colSums(is.na(trainset)) == 0] 
testset <- testset[, colSums(is.na(testset)) == 0] 
```  
Next, we get rid of some columns that do not contribute much to the accelerometer measurements.
```{r, cache = T}
classe <- trainset$classe
trainRemove <- grepl("^X|timestamp|window", names(trainset))
trainset <- trainset[, !trainRemove]
trainCleaned <- trainset[, sapply(trainset, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testset))
testset <- testset[, !testRemove]
testCleaned <- testset[, sapply(testset, is.numeric)]
```
Now, the cleaned training data set contains 19622 observations with 53 variables and the testing data set contains 20 observations with 53 variables. The "classe" variable is still in the cleaned training set.

### Data Splitting
Then, we can split the cleaned training set into a pure training data set (70%) and a validation data set (30%). We will use the validation data set to conduct cross validation in future steps.  
```{r, cache = T}
# Set a seed
set.seed(500) 
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainset <- trainCleaned[inTrain, ]
testset <- trainCleaned[-inTrain, ]
```

## Choosing the Prediction Algorithm:

The predictor we intend to use for this classification problem is a random forest. The reasons we are employing a random forest are:

- After filtering out sparse variables there are still 52 input variables to work with. Random forests are particularly well suited to handle a large number of inputs, especially when the interactions between variables are unknown.
- A random forest has a built in cross-validation component that gives an unbiased estimate of the forest's out-of-sample (OOB) error rate. This OOB error rate can be helpful in tuning the forest's parameters.
- A random forest can be used to estimate variable importance. This is especially helpful if the goal is to trim down the inputs into a more parsimonious set.
- A random forest can handle unscaled variables and categorical variables, which reduces the need for cleaning and transforming variables which are steps that can be subject to overfitting and noise.
- Individual trees can be pulled out of the random forest and examined. This allows for decent intuition into how the predictor is arriving at its predicted classifications.
- The random forest's classification output can be expressed as a probability (# trees w classification / total # of trees) which can be used as a confidence estimate for each classification.


## Buidling the Model

In order to build a predictive model, we first investigate the features importance.
The correlation matrix between the features is shown below.
```{r, cache = T}
corrPlot <- cor(trainset[, -length(names(trainset))])
corrplot(corrPlot, method="color")
```

Also, the decision tree can be used for visualizing the feature importance as follows.
```{r, cache = T}
treeModel <- rpart(classe ~ ., data=trainset, method="class")
prp(treeModel) # fast plot
```

Based on this information, we fit a predictive model for activity recognition using the random forest algorithm.
We use 5-fold cross validation to apply the algorithm.  
```{r, cache = T}
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainset, method="rf", trControl=controlRf, ntree=250)
modelRf
```
Then, we estimate the performance of the model on the validation data set.  
```{r, cache = T}
predictRf <- predict(modelRf, testset)
confusionMatrix(testset$classe, predictRf)
```
```{r, cache = T}
accuracy <- postResample(predictRf, testset$classe)
accuracy
oose <- 1 - as.numeric(confusionMatrix(testset$classe, predictRf)$overall[1])
oose
```


## Expectation for Out-of-sample Error

The trained random forest will now use the testing sub data set to etimate the out of sample error rate. The sub data error set was origionally apart of the total training set, but it was carved out a left untouched during variable selection, training and tuning of the random forest. Therefore the testing subset should be an unbiased estimate of the random forest's prediction accuracy.
Based on the missclassificaiton rate on the testing subset, an unbiased estimate of the random forest's out-of-sample error rate is 0.58%.
 



